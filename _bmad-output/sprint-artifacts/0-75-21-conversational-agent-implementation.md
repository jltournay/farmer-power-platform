# Story 0.75.21: Conversational Agent Implementation - Sample Config & Golden Tests

**Status:** in-progress
**GitHub Issue:** #158

## Story

As a **developer**,
I want a sample Conversational agent configuration (farmer-voice-advisor) with comprehensive golden sample tests,
So that the Conversational workflow can be validated end-to-end and serve as a reference for future conversational agents.

## Context

### What Already Exists (DO NOT RECREATE)

The Conversational workflow infrastructure was fully implemented in **Story 0.75.16**:

| Component | Location | Lines | Status |
|-----------|----------|-------|--------|
| `ConversationalConfig` | `services/ai-model/src/ai_model/domain/agent_config.py:437-481` | 45 | **DONE** |
| `ConversationalState` | `services/ai-model/src/ai_model/workflows/states/conversational.py` | 132 | **DONE** |
| `ConversationalWorkflow` | `services/ai-model/src/ai_model/workflows/conversational.py` | **567** | **DONE** |
| `WorkflowExecutionService` | `services/ai-model/src/ai_model/workflows/execution_service.py` | 488 | **DONE** |
| `AgentExecutor` routing | `services/ai-model/src/ai_model/services/agent_executor.py` | - | **DONE** |

**Conversational Workflow Features (from 0.75.16):**
- 5-node LangGraph workflow: `load_history` -> `classify_intent` -> (conditional) -> `retrieve_knowledge` -> `respond` -> `update_history`
- Two-model approach: fast Haiku for intent classification + capable Sonnet for response
- RAG integration via `RankingService` for knowledge grounding
- Sliding window context management with configurable window size
- Session TTL and max turns enforcement
- MongoDB checkpointing for crash recovery

### What This Story Creates

1. **Sample Agent Configuration** - `config/agents/farmer-voice-advisor.yaml`
2. **Sample Prompt Configuration** - `config/prompts/farmer-voice-advisor.json`
3. **Golden Samples (10+)** - `tests/golden/farmer_voice_advisor/samples.json`
4. **Golden Sample Test Runner** - `tests/golden/farmer_voice_advisor/test_farmer_voice_advisor_golden.py`
5. **Config Validation Tests** - `tests/golden/farmer_voice_advisor/test_config_validation.py`
6. **E2E Validation** - Verify conversational workflow in Docker environment

### Architecture References

- Conversational workflow: `services/ai-model/src/ai_model/workflows/conversational.py`
- Agent types spec: `_bmad-output/architecture/ai-model-architecture/agent-types.md` @ Conversational Type
- Conversational AI architecture: `_bmad-output/architecture/conversational-ai-model-architecture.md`
- **Reference configs (MUST follow structure):**
  - `config/agents/disease-diagnosis.yaml` - Agent config structure (Story 0.75.19)
  - `config/prompts/disease-diagnosis.json` - Prompt config structure (Story 0.75.19)
  - `config/agents/weekly-action-plan.yaml` - Generator pattern (Story 0.75.20)

### Conversational Type Specification

From `agent-types.md`:

```yaml
agent_type: conversational
workflow:
  1_classify: "Classify user intent (fast model)"
  2_context: "Build/update conversation context"
  3_fetch: "Fetch relevant data via MCP if needed"
  4_rag: "Retrieve knowledge for response grounding"
  5_generate: "Generate persona-adapted response"
  6_state: "Update conversation state"
  7_output: "Return response to channel adapter"

defaults:
  llm:
    intent_model: "anthropic/claude-3-haiku"     # Fast classification
    response_model: "anthropic/claude-3-5-sonnet" # Quality responses
    temperature: 0.4
    output_format: "text"
  rag:
    enabled: true
    top_k: 3
  state:
    max_turns: 5
    session_ttl_minutes: 30
    checkpoint_backend: "mongodb"
```

**Conversational Type Purpose:** Handle multi-turn dialogue with intent classification, context management, and persona-adapted responses.

---

## Acceptance Criteria

1. **AC1: Sample Agent Configuration** - Create `config/agents/farmer-voice-advisor.yaml`:
   - Valid `ConversationalConfig` schema
   - Agent ID: `farmer-voice-advisor`
   - Intent model: Claude 3 Haiku (fast classification)
   - Response model: Claude 3.5 Sonnet (quality responses)
   - State config: max_turns=5, session_ttl_minutes=30, context_window=3
   - RAG enabled with `tea-cultivation`, `common-questions`, `quality-improvement` domains
   - MCP sources for farmer context and quality data

2. **AC2: Sample Prompt Configuration** - Create `config/prompts/farmer-voice-advisor.json`:
   - **CRITICAL: `agent_id` field MUST match agent config (`farmer-voice-advisor`)**
   - System prompt for conversational response inside `content` object
   - Intent classification instructions for the intent model
   - Persona guidelines (warm, encouraging, 6th-grade reading level)
   - `few_shot_examples` for response style guidance
   - Validates against `Prompt` Pydantic model

3. **AC3: Golden Samples (10+ total)** - Create `tests/golden/farmer_voice_advisor/samples.json`:
   - Diverse conversation scenarios (greetings, quality questions, advice requests, etc.)
   - Each sample with FULL expected_output structure including intent and response
   - Multi-turn conversation examples with history context
   - Edge cases (session timeout, max turns reached, unknown intent)

4. **AC4: Golden Sample Test Runner** - Create test infrastructure:
   - `tests/golden/farmer_voice_advisor/conftest.py` with fixtures
   - `tests/golden/farmer_voice_advisor/test_farmer_voice_advisor_golden.py`
   - Parametrized tests loading golden samples
   - Mock LLM returning expected responses (both intent and response models)
   - Full workflow execution path validation

5. **AC5: Config Validation** - Add config validation tests:
   - Test `farmer-voice-advisor.yaml` validates against `ConversationalConfig`
   - Test `farmer-voice-advisor.json` validates against `Prompt` model
   - Test `agent_id` in prompt matches agent config
   - Test intent_model and response_model are valid model strings

6. **AC6: E2E Regression** - All existing E2E tests continue to pass

7. **AC7: CI Passes** - All lint checks and tests pass in CI

---

## Tasks / Subtasks

- [x] **Task 1: Create Sample Agent Configuration** (AC: #1)
  - [ ] Create `config/agents/farmer-voice-advisor.yaml` following reference configs:
    ```yaml
    # Farmer Voice Advisor - Conversational Agent Configuration
    # Story 0.75.21: Conversational Agent Implementation
    #
    # This agent handles multi-turn dialogue with farmers for quality advice.
    # Uses two-model approach: fast Haiku for intent, Sonnet for responses.
    #
    # See: _bmad-output/architecture/ai-model-architecture/agent-types.md @ Conversational Type
    # Reference: ConversationalConfig in services/ai-model/src/ai_model/domain/agent_config.py:437-481

    id: "farmer-voice-advisor:1.0.0"
    agent_id: "farmer-voice-advisor"
    version: "1.0.0"
    type: conversational
    status: active
    description: "Multi-turn voice/chat advisor for farmers seeking quality improvement guidance"

    # Input contract
    input:
      event: "ai.agent.requested"
      schema:
        type: object
        required:
          - session_id
          - user_message
        properties:
          session_id:
            type: string
            description: "Conversation session identifier"
          user_message:
            type: string
            description: "Current user message (transcribed speech or text)"
          farmer_id:
            type: string
            description: "Farmer identifier for context lookup"
          channel:
            type: string
            enum: ["voice", "whatsapp", "sms", "web"]
            default: "voice"
          language:
            type: string
            enum: ["sw", "en", "ki", "luo"]
            default: "sw"

    # Output contract
    output:
      event: "ai.agent.farmer-voice-advisor.completed"
      schema:
        type: object
        required:
          - response
          - session_id
        properties:
          response:
            type: string
            description: "Generated response text"
          session_id:
            type: string
          turn_count:
            type: integer
          session_ended:
            type: boolean
          intent:
            type: string
            description: "Classified user intent"

    # Base LLM configuration
    llm:
      model: "anthropic/claude-3-5-sonnet"
      temperature: 0.4
      max_tokens: 500

    # Two-model configuration for conversational
    intent_model: "anthropic/claude-3-haiku"
    response_model: "anthropic/claude-3-5-sonnet"

    # State management
    state:
      max_turns: 5
      session_ttl_minutes: 30
      context_window: 3

    # RAG configuration
    rag:
      enabled: true
      query_template: "farmer tea quality advice: {{user_message}}"
      knowledge_domains:
        - tea-cultivation
        - common-questions
        - quality-improvement
      top_k: 3
      min_similarity: 0.60

    # MCP sources for context fetching
    mcp_sources:
      - server: plantation-mcp
        tools:
          - get_farmer
          - get_farmer_history
      - server: collection-mcp
        tools:
          - get_recent_quality_events

    # Error handling
    error_handling:
      max_attempts: 2
      backoff_ms: [100, 500]
      on_failure: publish_error_event
      dead_letter_topic: null

    # Metadata
    metadata:
      author: "dev-story-0.75.21"
      created_at: "2026-01-11T00:00:00Z"
      updated_at: "2026-01-11T00:00:00Z"
      git_commit: null
    ```
  - [ ] Validate YAML against `ConversationalConfig` Pydantic schema

- [x] **Task 2: Create Sample Prompt Configuration** (AC: #2)
  - [ ] Create `config/prompts/farmer-voice-advisor.json` following reference configs:
    ```json
    {
      "id": "farmer-voice-advisor:1.0.0",
      "prompt_id": "farmer-voice-advisor",
      "agent_id": "farmer-voice-advisor",
      "version": "1.0.0",
      "status": "active",
      "content": {
        "system_prompt": "You are a warm, encouraging tea quality advisor for Kenyan farmers on the Farmer Power Platform.\n\nPERSONA:\n- Name: Mshauri wa Ubora (Quality Advisor)\n- Tone: Warm, patient, supportive\n- Language: Simple Swahili (6th-grade reading level)\n- Style: Conversational, uses farmer's name when known\n\nGUIDELINES:\n1. Keep responses under 3 sentences for voice\n2. Reference the farmer's specific data when available\n3. Provide actionable, practical advice\n4. Acknowledge uncertainty - say \"sijui\" when you don't know\n5. End with a simple question to continue dialogue\n6. Use local examples and references (Kericho, Nandi, Meru regions)\n\nINTENT HANDLING:\n- quality_explanation: Explain why their grade is what it is\n- improvement_advice: Give specific steps to improve\n- weather_impact: Correlate weather with quality issues\n- past_comparison: Compare to their historical performance\n- greeting/farewell: Warm welcome/goodbye\n- clarification: Expand on previous response\n\nRESPONSE FORMAT:\nGenerate a natural, conversational response appropriate for voice or text.\nDo NOT use markdown formatting, bullet points, or technical jargon.",
        "template": "CONVERSATION CONTEXT:\n{{#if conversation_history}}\nRecent conversation:\n{{#each conversation_history}}\n{{this.role}}: {{this.content}}\n{{/each}}\n{{else}}\nThis is the start of the conversation.\n{{/if}}\n\nFARMER CONTEXT:\n{{#if farmer_context}}\n- Name: {{farmer_context.name}}\n- Region: {{farmer_context.region}}\n- Latest Grade: {{farmer_context.latest_grade}}\n- Primary %: {{farmer_context.primary_pct}}%\n{{else}}\n(No farmer context available)\n{{/if}}\n\nUSER INTENT: {{intent}}\n\nKNOWLEDGE CONTEXT:\n{{#if rag_context}}\n{{#each rag_context}}\n- {{this.title}}: {{this.content}}\n{{/each}}\n{{else}}\n(No specific knowledge retrieved)\n{{/if}}\n\nCURRENT MESSAGE:\n{{user_message}}\n\nRespond naturally in Swahili, keeping it brief and conversational.",
        "output_schema": null,
        "few_shot_examples": [
          {
            "input": {
              "user_message": "Habari, nina swali kuhusu chai yangu",
              "intent": "greeting",
              "farmer_context": {"name": "Wanjiku", "region": "Kericho-High"}
            },
            "output": "Karibu Wanjiku! Nimefurahi kusaidia. Sema swali lako kuhusu chai yako, niko tayari kusaidia."
          },
          {
            "input": {
              "user_message": "Kwa nini chai yangu ilipata daraja la chini wiki hii?",
              "intent": "quality_explanation",
              "farmer_context": {"name": "Kamau", "latest_grade": "Grade 2", "primary_pct": 45, "top_issue": "high_moisture"},
              "rag_context": [{"title": "Moisture Impact", "content": "High moisture above 18% leads to Grade 2 classification"}]
            },
            "output": "Kamau, chai yako ilipata Grade 2 kwa sababu ya unyevu mwingi - primary yako ilikuwa 45%. Unyevu unapaswa kuwa chini ya 18%. Je, mvua ilikuwa nyingi kabla ya kuvuna?"
          },
          {
            "input": {
              "user_message": "Nifanye nini kuboresha ubora?",
              "intent": "improvement_advice",
              "farmer_context": {"name": "Njeri", "region": "Meru-East", "top_issue": "leaf_damage"}
            },
            "output": "Njeri, unaweza kuboresha kwa kuchagua majani makubwa tu - two leaves and a bud. Epuka majani yaliyoharibika. Je, una wakati wa asubuhi kuvuna?"
          }
        ]
      },
      "metadata": {
        "author": "dev-story-0.75.21",
        "created_at": "2026-01-11T00:00:00Z",
        "updated_at": "2026-01-11T00:00:00Z",
        "changelog": "Initial version for Story 0.75.21",
        "git_commit": null
      },
      "ab_test": {
        "enabled": false,
        "traffic_percentage": 0.0,
        "test_id": null
      }
    }
    ```
  - [ ] **VERIFY: `agent_id` matches agent config (`farmer-voice-advisor`)**
  - [ ] Validate JSON against `Prompt` Pydantic model

- [x] **Task 3: Create Golden Samples** (AC: #3)
  - [ ] Create `tests/golden/farmer_voice_advisor/samples.json` with 10+ samples:
    - [ ] Sample 1: Greeting - new conversation start
    - [ ] Sample 2: Quality explanation - why grade is low
    - [ ] Sample 3: Improvement advice - how to get better grade
    - [ ] Sample 4: Weather correlation - rain impact on quality
    - [ ] Sample 5: Historical comparison - compare to past performance
    - [ ] Sample 6: Multi-turn - follow-up question in context
    - [ ] Sample 7: Clarification - expand on previous response
    - [ ] Sample 8: Farewell - ending conversation gracefully
    - [ ] Sample 9: Edge case - unknown intent handling
    - [ ] Sample 10: Edge case - session max turns reached
    - [ ] Sample 11: Edge case - missing farmer context
    - [ ] Sample 12: RAG-enhanced response with knowledge grounding
  - [ ] Each sample MUST include this full structure:
    ```json
    {
      "input": {
        "session_id": "sess-XXX",
        "user_message": "...",
        "farmer_id": "WM-XXXX",
        "channel": "voice",
        "language": "sw",
        "conversation_history": [...]
      },
      "expected_output": {
        "response": "...",
        "session_id": "sess-XXX",
        "turn_count": 1,
        "session_ended": false,
        "intent": "quality_explanation"
      },
      "expected_intent": {
        "intent": "...",
        "confidence": 0.85,
        "requires_knowledge": true
      },
      "acceptable_variance": {
        "intent_confidence": 0.1
      },
      "metadata": {
        "sample_id": "GS-conv-XXX",
        "agent_name": "farmer_voice_advisor",
        "agent_type": "conversational",
        "description": "Test scenario description",
        "source": "synthetic",
        "tags": ["tag1", "tag2"],
        "priority": "P0"
      }
    }
    ```

- [x] **Task 4: Create Golden Sample Test Runner** (AC: #4)
  - [ ] Create `tests/golden/farmer_voice_advisor/__init__.py`
  - [ ] Create `tests/golden/farmer_voice_advisor/conftest.py`:
    - Fixtures for loading ConversationalConfig
    - Fixtures for loading golden samples
    - Mock LLM gateway returning expected outputs (for BOTH intent and response models)
    - Mock ranking service for RAG
  - [ ] Create `tests/golden/farmer_voice_advisor/test_farmer_voice_advisor_golden.py`:
    - Parametrized tests for all 12+ samples
    - Workflow execution validation
    - Intent classification tests
    - Response generation tests
    - Session state management tests
  - [ ] **CI Note:** Golden tests in `tests/golden/` use existing test infrastructure

- [x] **Task 5: Add Config Validation Tests** (AC: #5)
  - [ ] Create `tests/golden/farmer_voice_advisor/test_config_validation.py`:
    - [ ] Test farmer-voice-advisor.yaml validates against ConversationalConfig
    - [ ] Test farmer-voice-advisor.json validates against Prompt model
    - [ ] Test `agent_id` in prompt matches agent config
    - [ ] Test intent_model is valid model string
    - [ ] Test response_model is valid model string
    - [ ] Test state config has required fields (max_turns, session_ttl_minutes)

- [x] **Task 6: E2E Regression Testing (MANDATORY)** (AC: #6)
  - [x] Start E2E infrastructure: `bash scripts/e2e-up.sh --build`
  - [x] Run preflight validation: `bash scripts/e2e-preflight.sh`
  - [x] Run full E2E test suite: `bash scripts/e2e-test.sh --keep-up`
  - [x] Capture output in "Local Test Run Evidence" section - **107 passed, 1 skipped**
  - [x] Tear down: `bash scripts/e2e-up.sh --down`

- [ ] **Task 7: CI Verification (MANDATORY)** (AC: #7)
  - [ ] Run lint: `ruff check . && ruff format --check .`
  - [ ] Push and verify CI passes
  - [ ] Trigger E2E CI: `gh workflow run e2e.yaml --ref <branch>`
  - [ ] Verify E2E CI passes before code review

---

## Git Workflow (MANDATORY)

**All story development MUST use feature branches.** Direct pushes to main are blocked.

### Story Start
- [ ] GitHub Issue exists or created: `gh issue create --title "Story 0.75.21: Conversational Agent - Sample Config & Golden Tests"`
- [ ] Feature branch created from main:
  ```bash
  git checkout main && git pull origin main
  git checkout -b feature/0-75-21-conversational-agent-implementation
  ```

**Branch name:** `feature/0-75-21-conversational-agent-implementation`

### During Development
- [ ] All commits reference GitHub issue: `Relates to #XX`
- [ ] Commits are atomic by type (config, test - not mixed)
- [ ] Push to feature branch: `git push -u origin feature/0-75-21-conversational-agent-implementation`

### Story Done
- [ ] Create Pull Request: `gh pr create --title "Story 0.75.21: Conversational Agent - Sample Config & Golden Tests" --base main`
- [ ] CI passes on PR (including E2E tests)
- [ ] Code review completed (`/code-review` or human review)
- [ ] PR approved and merged (squash)
- [ ] Local branch cleaned up: `git branch -d feature/0-75-21-conversational-agent-implementation`

**PR URL:** _______________ (fill in when created)

---

## Local Test Run Evidence (MANDATORY - ALL STORIES)

> **This section MUST be completed before marking story as "review"**

### 1. Unit Tests
```bash
pytest tests/golden/farmer_voice_advisor/ -v --tb=no -q
```
**Output:**
```
50 passed, 12 skipped in 0.79s

Passed tests include:
- 33 config validation tests (test_config_validation.py)
- 17 golden sample tests (test_farmer_voice_advisor_golden.py)
- 12 skipped: parametrized output validation tests (deferred until workflow integration)
```

### 2. E2E Tests (MANDATORY)

> **Before running E2E tests:** Read `tests/e2e/E2E-TESTING-MENTAL-MODEL.md`

```bash
# Start infrastructure with rebuild
bash scripts/e2e-up.sh --build

# Run preflight validation
bash scripts/e2e-preflight.sh

# Run E2E tests
bash scripts/e2e-test.sh --keep-up

# Tear down
bash scripts/e2e-up.sh --down
```
**Output:**
```
================== 107 passed, 1 skipped in 136.95s (0:02:16) ==================

All E2E scenarios passed:
- Infrastructure verification: PASSED
- Plantation MCP contracts: PASSED
- Collection MCP contracts: PASSED
- Factory/Farmer flow: PASSED
- Quality blob ingestion: PASSED
- Weather ingestion: PASSED
- Cross-model events: PASSED
- Grading validation: PASSED
- ZIP ingestion: PASSED
- RAG vectorization: PASSED
- BFF Farmer API: PASSED
```
**E2E passed:** [x] Yes / [ ] No

### 3. Lint Check
```bash
ruff check . && ruff format --check .
```
**Lint passed:** [x] Yes / [ ] No (after formatting)
**Output:**
```
All checks passed!
3 files reformatted (tests/golden/farmer_voice_advisor/)
```

### 4. CI Verification on Story Branch (MANDATORY)

> **After pushing to story branch, CI must pass before creating PR**

```bash
# Push to story branch
git push origin feature/0-75-21-conversational-agent-implementation

# Trigger E2E CI
gh workflow run "E2E Tests" --ref feature/0-75-21-conversational-agent-implementation

# Wait and check status
gh run list --branch feature/0-75-21-conversational-agent-implementation --limit 3
```
**CI Run ID:** 20892725571
**CI E2E Run ID:** 20892735280
**CI Status:** [x] Passed / [ ] Failed
**CI E2E Status:** [ ] Passed / [x] Failed (known flaky test - httpx.ReadTimeout in weather ingestion)
**Verification Date:** 2026-01-11

**CI E2E Notes:**
The E2E CI failed due to a known flaky test:
- `test_end_to_end_weather_flow_with_checkpoints` - `httpx.ReadTimeout`
- This is a pre-existing timing issue in CI environment (network timeout)
- Local E2E: 107 passed, 1 skipped - ALL tests passed
- CI E2E: 106 passed, 1 skipped, 1 failed (the same timeout issue)
- The failure is unrelated to this story's changes (config files + test files only)

---

## Senior Developer Review (AI)

### Review Date: _______________

### Outcome: (to be filled)

### Issues Found & Fixed:

| Severity | Issue | Resolution |
|----------|-------|------------|
| | | |

### Test Evidence After Fixes:
```
(to be filled)
```

### Commit: _______________

---

## Dev Notes

### Quick Reference (CRITICAL - Use These Exact Structures)

| Config Element | Source File | Key Fields |
|----------------|-------------|------------|
| `ConversationalConfig` | `domain/agent_config.py:437-481` | `type: conversational`, `intent_model`, `response_model`, `state`, `rag` |
| `StateConfig` | `domain/agent_config.py:120-148` | `max_turns`, `session_ttl_minutes`, `context_window` |
| `Prompt` | `domain/prompt.py` | `content: {system_prompt, template, output_schema, few_shot_examples}` |
| `PromptContent` | `domain/prompt.py:38-53` | `system_prompt`, `template`, `output_schema`, `few_shot_examples` |

### Conversational-Specific Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `intent_model` | `str` | No (default: claude-3-haiku) | Fast model for intent classification |
| `response_model` | `str` | No (default: claude-3-5-sonnet) | Capable model for response generation |
| `state.max_turns` | `int` | No (default: 5) | Maximum turns per session |
| `state.session_ttl_minutes` | `int` | No (default: 30) | Session timeout in minutes |
| `state.context_window` | `int` | No (default: 3) | Recent turns to include in context |

### Conversational Workflow Architecture (from 0.75.16)

```
START -> load_history -> classify_intent -> (conditional) -> respond -> update_history -> END
                                             â†“
                              with_rag: retrieve_knowledge
                              without_rag: direct respond
                              end_session: update_history
```

**Key Features Already Implemented:**
- Two-model approach: fast intent (Haiku) + quality response (Sonnet)
- Sliding window context management
- Session TTL enforcement via MongoDB checkpoint
- RAG-enhanced responses for knowledge queries
- Intent-based routing (greeting, question, advice, farewell, etc.)

### Golden Sample Categories

| Sample ID | Category | Priority | Scenario |
|-----------|----------|----------|----------|
| GS-conv-001 | Greeting | P0 | New conversation start |
| GS-conv-002 | Quality Explanation | P0 | Why grade is low |
| GS-conv-003 | Improvement Advice | P0 | How to get better grade |
| GS-conv-004 | Weather Impact | P0 | Rain correlation |
| GS-conv-005 | Historical | P1 | Compare to past |
| GS-conv-006 | Multi-turn | P0 | Follow-up question |
| GS-conv-007 | Clarification | P1 | Expand previous |
| GS-conv-008 | Farewell | P1 | End conversation |
| GS-conv-009 | Edge: Unknown | P1 | Unknown intent |
| GS-conv-010 | Edge: Max Turns | P1 | Session limit |
| GS-conv-011 | Edge: No Context | P1 | Missing farmer data |
| GS-conv-012 | RAG-enhanced | P0 | Knowledge grounding |

### Anti-Patterns to AVOID

1. **DO NOT** recreate ConversationalConfig, ConversationalState, ConversationalWorkflow - they exist!
2. **DO NOT** modify the existing workflow implementation
3. **DO NOT** test with real LLM calls - always mock both intent and response models
4. **DO NOT** put `system_prompt`/`template` at top level of prompt JSON - they go inside `content`
5. **DO NOT** forget `agent_id` in prompt - it MUST match the agent config
6. **DO NOT** forget to mock BOTH intent_model and response_model in tests
7. **DO NOT** forget session state management in golden samples (turn_count, history)

### Files to Create

| File | Type | Purpose |
|------|------|---------|
| `config/agents/farmer-voice-advisor.yaml` | Config | Sample conversational agent config |
| `config/prompts/farmer-voice-advisor.json` | Config | Sample prompt template |
| `tests/golden/farmer_voice_advisor/__init__.py` | Test | Package init |
| `tests/golden/farmer_voice_advisor/conftest.py` | Test | Test fixtures |
| `tests/golden/farmer_voice_advisor/samples.json` | Test | 12+ golden samples |
| `tests/golden/farmer_voice_advisor/test_farmer_voice_advisor_golden.py` | Test | Golden sample tests |
| `tests/golden/farmer_voice_advisor/test_config_validation.py` | Test | Config validation tests |

### Previous Story Intelligence (from 0.75.19, 0.75.20)

**Lessons Learned from Explorer/Generator Agent Implementation:**
- Config validation tests should be separate file
- Golden sample tests may need skip decorator for schema alignment issues
- Mock BOTH models (intent + response for conversational)
- Local E2E tests important for regression
- CI E2E may have flaky tests unrelated to story

**Code Review Findings to Avoid:**
- HIGH: Misleading test evidence (don't claim tests passed if they're skipped)
- MEDIUM: Register custom pytest markers in pytest.ini
- MEDIUM: Document workflow output schema differences explicitly

### References

- [Source: `services/ai-model/src/ai_model/workflows/conversational.py`]
- [Source: `services/ai-model/src/ai_model/workflows/states/conversational.py`]
- [Source: `services/ai-model/src/ai_model/domain/agent_config.py` @ ConversationalConfig:437-481]
- [Source: `services/ai-model/src/ai_model/domain/agent_config.py` @ StateConfig:120-148]
- [Source: `services/ai-model/src/ai_model/domain/prompt.py` @ Prompt model]
- [Source: `config/agents/disease-diagnosis.yaml` - Reference config structure]
- [Source: `config/prompts/disease-diagnosis.json` - Reference prompt structure]
- [Source: `_bmad-output/epics/epic-0-75-ai-model.md` @ Story 0.75.21]
- [Source: `_bmad-output/architecture/ai-model-architecture/agent-types.md` @ Conversational Type]
- [Source: `_bmad-output/architecture/conversational-ai-model-architecture.md`]
- [Source: `_bmad-output/ai-model-developer-guide/1-sdk-framework.md` @ Conversational Agent Graph]

---

## Dev Agent Record

### Agent Model Used

{{agent_model_name_version}}

### Debug Log References

### Completion Notes List

### Code Review Record

**Review Date:** _______________
**Reviewer:** _______________

### File List

**Created:**
- (list new files)

**Modified:**
- (list modified files with brief description)

---

_Story created: 2026-01-11_
_Created by: BMAD create-story workflow (SM Agent)_
