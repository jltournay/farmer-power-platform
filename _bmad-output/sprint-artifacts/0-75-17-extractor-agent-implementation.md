# Story 0.75.17: Extractor Agent Implementation

**Status:** ready-for-dev
**GitHub Issue:** <!-- Auto-created by dev-story workflow -->

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

As a **developer building AI-powered data extraction**,
I want the Extractor agent type fully implemented with golden sample testing,
So that structured data can be reliably extracted from unstructured input with validated accuracy.

## Context

Story 0.75.16 implemented the base `ExtractorWorkflow` class with the linear LangGraph pipeline:
`fetch_data` → `extract` → `validate` → `normalize` → `output`

Story 0.75.16b wires the event subscriber to the `WorkflowExecutionService` - enabling agents to be invoked via DAPR events.

This story completes the Extractor agent implementation by:
1. Creating a sample extractor agent configuration (loaded into MongoDB)
2. Implementing golden sample testing framework for extractors
3. Validating the workflow works end-to-end with real LLM calls

**Key Principle:** Agent types are generic code. Specific extractors (like `qc-event-extractor` or `weather-extractor`) are **configurations** stored in MongoDB and deployed via CLI (`fp-agent-config`, `fp-prompt-config`) - NO NEW WORKFLOW CODE is needed for new extraction use cases.

**Architecture References:**
- Extractor Agent Type: `_bmad-output/architecture/ai-model-architecture/agent-types.md`
- Agent Development Guide: `_bmad-output/ai-model-developer-guide/3-agent-development.md`
- Developer Guide SDK: `_bmad-output/ai-model-developer-guide/1-sdk-framework.md`

## Acceptance Criteria

1. **AC1: Sample Extractor Agent Configuration** - Create and deploy a sample extractor agent:
   - YAML configuration file: `config/agents/extractors/sample-extractor.yaml` (for CLI deployment)
   - System prompt file: `config/prompts/extractors/sample/system.md`
   - Template prompt file: `config/prompts/extractors/sample/template.md`
   - Agent extracts: `name`, `date`, `amount`, `category` from unstructured text
   - Uses `anthropic/claude-3-haiku` for fast, cheap extraction
   - Temperature: 0.1 (deterministic)
   - **Deploy to MongoDB** via `fp-agent-config deploy` and `fp-prompt-config deploy`

2. **AC2: Golden Sample Test Framework** - Create testing infrastructure:
   - `tests/golden/extractor/conftest.py` with fixtures for extractor testing
   - `tests/golden/extractor/samples.json` with minimum 10 synthetic samples
   - Each sample: input text + expected extraction output + acceptable variance
   - Synthetic samples generated by dev agent (no external LLM dependency for sample generation)

3. **AC3: Golden Sample Test Suite** - Implement accuracy validation:
   - `tests/golden/extractor/test_extractor_golden.py`
   - Test cases for each synthetic sample
   - Accuracy threshold: >=90% of samples must pass
   - Field-level accuracy tracking (which fields fail most often)
   - Mock LLM responses for unit tests, real LLM for golden sample tests

4. **AC4: WorkflowExecutionService Integration** - Verify end-to-end execution:
   - Unit tests for config loading → prompt loading → workflow execution → result
   - Test error handling paths (missing config, LLM failure, validation failure)

5. **AC5: E2E Regression** - All existing E2E tests continue to pass:
   - Run full E2E suite with `--build` flag
   - No modifications to existing E2E test files

6. **AC6: CI Passes** - All lint checks and tests pass in CI

## Tasks / Subtasks

- [ ] **Task 1: Create Sample Extractor Agent Configuration** (AC: #1)
  - [ ] Create `services/ai-model/config/agents/extractors/sample-extractor.yaml`:
    ```yaml
    agent:
      id: "sample-extractor"
      type: extractor
      version: "1.0.0"
      description: "Sample extractor for testing - extracts structured data from text"

      input:
        schema:
          required: [text]
          optional: [context]

      output:
        schema:
          fields: [name, date, amount, category]

      llm:
        model: "anthropic/claude-3-haiku"
        temperature: 0.1
        max_tokens: 500

      extraction_schema:
        required_fields: [name, amount]
        optional_fields: [date, category]
        field_types:
          name: string
          date: string
          amount: number
          category: string

      normalization_rules:
        - field: name
          transform: title
        - field: category
          transform: lowercase
    ```
  - [ ] Create `services/ai-model/config/prompts/extractors/sample/system.md`
  - [ ] Create `services/ai-model/config/prompts/extractors/sample/template.md`
  - [ ] Add `prompt_id` field to agent config referencing the prompt
  - [ ] **Deploy to MongoDB** (for manual testing and E2E):
    ```bash
    # Deploy prompt first (agent config references it)
    fp-prompt-config deploy config/prompts/extractors/sample/

    # Deploy agent config
    fp-agent-config deploy config/agents/extractors/sample-extractor.yaml
    ```
  - [ ] Add seed data for E2E tests: `tests/e2e/infrastructure/seed/ai-model/sample-extractor.json`

- [ ] **Task 2: Create Golden Sample Test Framework** (AC: #2)
  - [ ] Create `tests/golden/extractor/__init__.py`
  - [ ] Create `tests/golden/extractor/conftest.py` with fixtures:
    ```python
    @pytest.fixture
    def golden_samples():
        """Load golden samples from JSON file."""
        ...

    @pytest.fixture
    def extractor_workflow(mock_llm_gateway):
        """Create ExtractorWorkflow instance for testing."""
        ...

    @pytest.fixture
    def sample_agent_config():
        """Load sample extractor agent config."""
        ...
    ```
  - [ ] Create `tests/golden/extractor/samples.json` with 10+ synthetic samples:
    - 3 samples: simple extraction (clear input → clear output)
    - 3 samples: partial extraction (some fields missing in input)
    - 2 samples: edge cases (unusual formats, special characters)
    - 2 samples: error cases (invalid input → graceful handling)

- [ ] **Task 3: Implement Golden Sample Test Suite** (AC: #3)
  - [ ] Create `tests/golden/extractor/test_extractor_golden.py`
  - [ ] Implement `test_golden_sample_accuracy()` - run all samples, track pass/fail
  - [ ] Implement `test_field_level_accuracy()` - track which fields fail most
  - [ ] Implement `test_extraction_determinism()` - same input → same output
  - [ ] Add `@pytest.mark.golden` marker for test isolation
  - [ ] Use mock LLM for unit tests, configure for real LLM in CI (optional)

- [ ] **Task 4: Full Pipeline Integration Tests** (AC: #4)
  - [ ] Add tests to `tests/unit/ai_model/workflows/test_extractor_integration.py`:
    - Test: Config loading from YAML
    - Test: Prompt loading from files
    - Test: Full workflow execution with mocked LLM
    - Test: Error handling for missing config
    - Test: Error handling for LLM failure

- [ ] **Task 5: E2E Regression Testing (MANDATORY)** (AC: #5)
  - [ ] Rebuild and start E2E infrastructure with `--build` flag
  - [ ] Verify Docker images were rebuilt (NOT cached) for ai-model
  - [ ] Run full E2E test suite
  - [ ] Capture output in "Local Test Run Evidence" section
  - [ ] Tear down infrastructure

- [ ] **Task 6: CI Verification** (AC: #6)
  - [ ] Run lint: `ruff check . && ruff format --check .`
  - [ ] Run unit tests locally
  - [ ] Push and verify CI passes
  - [ ] Trigger E2E CI workflow
  - [ ] Verify E2E CI passes before code review

## Git Workflow (MANDATORY)

**All story development MUST use feature branches.** Direct pushes to main are blocked.

### Story Start
- [ ] GitHub Issue exists or created: `gh issue create --title "Story 0.75.17: Extractor Agent Implementation"`
- [ ] Feature branch created from main:
  ```bash
  git checkout main && git pull origin main
  git checkout -b feature/0-75-17-extractor-agent-implementation
  ```

**Branch name:** `feature/0-75-17-extractor-agent-implementation`

### During Development
- [ ] All commits reference GitHub issue: `Relates to #XX`
- [ ] Commits are atomic by type (production, test, seed - not mixed)
- [ ] Push to feature branch: `git push -u origin feature/0-75-17-extractor-agent-implementation`

### Story Done
- [ ] Create Pull Request: `gh pr create --title "Story 0.75.17: Extractor Agent Implementation" --base main`
- [ ] CI passes on PR (including E2E tests)
- [ ] Code review completed (`/code-review` or human review)
- [ ] PR approved and merged (squash)
- [ ] Local branch cleaned up: `git branch -d feature/0-75-17-extractor-agent-implementation`

**PR URL:** _______________ (fill in when created)

---

## Local Test Run Evidence (MANDATORY - ALL STORIES)

> **This section MUST be completed before marking story as "review"**

### 1. Unit Tests
```bash
PYTHONPATH="libs/fp-common:libs/fp-proto/src:services/ai-model/src" pytest tests/unit/ai_model/ tests/golden/extractor/ -v -m "not golden"
```
**Output:**
```
(paste test summary here - e.g., "42 passed in 5.23s")
```

### 2. Golden Sample Tests
```bash
PYTHONPATH="libs/fp-common:libs/fp-proto/src:services/ai-model/src" pytest tests/golden/extractor/ -v -m golden
```
**Output:**
```
(paste golden sample test results here)
```
**Accuracy:** ___% (target: >=90%)

### 3. E2E Tests (MANDATORY)

> **Before running E2E tests:** Read `tests/e2e/E2E-TESTING-MENTAL-MODEL.md`

```bash
# Start infrastructure
docker compose -f tests/e2e/infrastructure/docker-compose.e2e.yaml up -d --build

# Wait for services, then run tests
PYTHONPATH="${PYTHONPATH}:.:libs/fp-proto/src" pytest tests/e2e/scenarios/ -v

# Tear down
docker compose -f tests/e2e/infrastructure/docker-compose.e2e.yaml down -v
```
**Output:**
```
(paste E2E test output here - story is NOT ready for review without this)
```
**E2E passed:** [ ] Yes / [ ] No

### 4. Lint Check
```bash
ruff check . && ruff format --check .
```
**Lint passed:** [ ] Yes / [ ] No

### 5. CI Verification on Story Branch (MANDATORY)

> **After pushing to story branch, CI must pass before creating PR**

```bash
# Push to story branch
git push origin feature/0-75-17-extractor-agent-implementation

# Wait ~30s, then check CI status
gh run list --branch feature/0-75-17-extractor-agent-implementation --limit 3
```
**CI Run ID:** _______________
**CI Status:** [ ] Passed / [ ] Failed
**E2E CI Run ID:** _______________
**E2E CI Status:** [ ] Passed / [ ] Failed
**Verification Date:** _______________

---

## Dev Notes

### Existing Code to REUSE (DO NOT RECREATE)

| Component | Location | Purpose |
|-----------|----------|---------|
| `ExtractorWorkflow` | `ai_model/workflows/extractor.py` | Base extractor workflow (Story 0.75.16) |
| `ExtractorState` | `ai_model/workflows/states/extractor.py` | TypedDict state for workflow |
| `WorkflowExecutionService` | `ai_model/workflows/execution_service.py` | Workflow executor |
| `LLMGateway` | `ai_model/llm/gateway.py` | LLM calls via OpenRouter |
| `AgentConfigCache` | `ai_model/services/agent_config_cache.py` | Agent config caching (Story 0.75.4) |
| `PromptCache` | `ai_model/services/prompt_cache.py` | Prompt caching (Story 0.75.4) |
| `create_node_wrapper` | `ai_model/workflows/base.py` | Node wrapper with telemetry |

### NEW Code to CREATE in this story

| Component | Location | Purpose |
|-----------|----------|---------|
| Sample agent config | `config/agents/extractors/sample-extractor.yaml` | Test extractor config |
| Sample prompts | `config/prompts/extractors/sample/` | System + template prompts |
| E2E seed data | `tests/e2e/infrastructure/seed/ai-model/` | Agent config + prompt for E2E |
| Golden sample tests | `tests/golden/extractor/` | Accuracy validation |

### Golden Sample Format

```json
{
  "samples": [
    {
      "id": "sample-001",
      "description": "Simple extraction with all fields present",
      "input": {
        "text": "John Smith paid $150.50 for groceries on December 15, 2025."
      },
      "expected_output": {
        "name": "John Smith",
        "date": "December 15, 2025",
        "amount": 150.50,
        "category": "groceries"
      },
      "variance": {
        "amount": 0.01,
        "date_format": "flexible"
      }
    }
  ]
}
```

### Synthetic Sample Generation

The dev agent generates all golden samples directly - NO external LLM call is needed for sample generation. Samples should cover:

| Category | Count | Examples |
|----------|-------|----------|
| Simple extraction | 3 | All fields clearly present in input |
| Partial extraction | 3 | Some fields missing, should return null |
| Edge cases | 2 | Unusual formats, special characters, long text |
| Error cases | 2 | Completely invalid input, empty input |

### Accuracy Calculation

```python
def calculate_accuracy(samples: list, results: list) -> float:
    """Calculate extraction accuracy across samples.

    A sample passes if all required fields match expected values
    within acceptable variance.
    """
    passed = 0
    for sample, result in zip(samples, results):
        if fields_match(sample["expected_output"], result, sample.get("variance", {})):
            passed += 1
    return (passed / len(samples)) * 100
```

### Field Matching Rules

| Field Type | Matching Rule |
|------------|---------------|
| string | Exact match (case-insensitive) OR normalized match |
| number | Within variance (default: 0.01) |
| date | Parsed date comparison (flexible format) |
| null | Both must be null |

### Test Isolation

Golden sample tests should be:
- **Isolated**: Each test runs independently
- **Deterministic**: Same input → same expected output
- **Fast**: Use mocked LLM for unit tests
- **Optional real LLM**: Configurable via env var for CI

### Anti-Patterns to AVOID

1. **DO NOT** create new workflow classes - use existing `ExtractorWorkflow`
2. **DO NOT** hardcode prompts in code - use configuration files
3. **DO NOT** skip golden sample tests - they validate accuracy
4. **DO NOT** use real LLM in unit tests - mock for speed
5. **DO NOT** modify `ExtractorWorkflow` unless absolutely necessary
6. **DO NOT** change Python version - stay on 3.12

### File Structure

```
services/ai-model/
├── config/
│   ├── agents/
│   │   └── extractors/
│   │       └── sample-extractor.yaml    # NEW: Sample agent config
│   └── prompts/
│       └── extractors/
│           └── sample/
│               ├── system.md            # NEW: System prompt
│               └── template.md          # NEW: Template prompt
└── src/ai_model/
    └── workflows/
        └── extractor.py                 # EXISTING: No changes expected

tests/
├── golden/
│   ├── __init__.py
│   └── extractor/
│       ├── __init__.py                  # NEW
│       ├── conftest.py                  # NEW: Test fixtures
│       ├── samples.json                 # NEW: Golden samples
│       └── test_extractor_golden.py     # NEW: Test suite
├── unit/ai_model/workflows/
│   └── test_extractor_integration.py    # NEW: Integration tests
└── e2e/infrastructure/seed/ai-model/
    └── sample-extractor.json            # NEW: E2E seed data
```

### Dependencies from Previous Stories

| Story | What it provides | Used in this story |
|-------|-----------------|-------------------|
| 0.75.1 | AI Model service scaffold | Base service structure |
| 0.75.4 | AgentConfigCache, PromptCache | Config and prompt loading |
| 0.75.5 | LLMGateway | LLM calls for extraction |
| 0.75.16 | ExtractorWorkflow, WorkflowExecutionService | Base workflow code |
| 0.75.16b | Event subscriber wiring | Event-driven execution (optional for this story) |

### Success Metrics (from Epic)

| Metric | Target | Validation |
|--------|--------|------------|
| Golden sample accuracy | >=90% | `test_golden_sample_accuracy` |
| Field-level tracking | All fields tracked | `test_field_level_accuracy` |
| Extraction determinism | Same input → same output | `test_extraction_determinism` |

### References

- [Source: `_bmad-output/epics/epic-0-75-ai-model.md` - Story 0.75.17 definition]
- [Source: `_bmad-output/architecture/ai-model-architecture/agent-types.md` - Extractor agent type]
- [Source: `_bmad-output/ai-model-developer-guide/3-agent-development.md` - Agent development]
- [Source: `_bmad-output/project-context.md` - Repository patterns and testing rules]
- [Source: `_bmad-output/test-design-system-level.md` - Golden sample testing strategy]
- [Source: Story 0.75.16 - Previous story patterns and learnings]

---

## Dev Agent Record

### Agent Model Used

{{agent_model_name_version}}

### Debug Log References

### Completion Notes List

### File List

**Created:**
- (list new files)

**Modified:**
- (list modified files with brief description)
