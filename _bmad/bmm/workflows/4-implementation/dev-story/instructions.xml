<workflow>
  <critical>The workflow execution engine is governed by: {project-root}/_bmad/core/tasks/workflow.xml</critical>
  <critical>You MUST have already loaded and processed: {installed_path}/workflow.yaml</critical>
  <critical>Communicate all responses in {communication_language} and language MUST be tailored to {user_skill_level}</critical>
  <critical>Generate all documents in {document_output_language}</critical>
  <critical>Only modify the story file in these areas: Tasks/Subtasks checkboxes, Dev Agent Record (Debug Log, Completion Notes), File List,
    Change Log, Status, and GitHub Issue (for traceability linking)</critical>
  <critical>Execute ALL steps in exact order; do NOT skip steps</critical>
  <critical>Absolutely DO NOT stop because of "milestones", "significant progress", or "session boundaries". Continue in a single execution
    until the story is COMPLETE (all ACs satisfied and all tasks/subtasks checked) UNLESS a HALT condition is triggered or the USER gives
    other instruction.</critical>
  <critical>Do NOT schedule a "next session" or request review pauses unless a HALT condition applies. Only Step 6 decides completion.</critical>
  <critical>User skill level ({user_skill_level}) affects conversation style ONLY, not code updates.</critical>
  <critical>GitHub Issue Linking: Use "Relates to #N" in commits (NOT "Fixes #N" or "Closes #N") - these auto-close the issue prematurely. Only use "Fixes #N" in the final commit when story status changes to "done".</critical>
  <critical>Commit Message Format: ALWAYS put issue reference in the SUBJECT LINE for reliable GitHub linking:
    Subject: "{{story_key}}: {{short_description}} (Relates to #{{github_issue_number}})"
    Example: "1-5: Farmer Communication Preferences (Relates to #12)"
    Body: Detailed description of changes
    Footer: ü§ñ Generated with [Claude Code] + Co-Authored-By
  </critical>

  <step n="1" goal="Find next ready story and load it" tag="sprint-status">
    <check if="{{story_path}} is provided">
      <action>Use {{story_path}} directly</action>
      <action>Read COMPLETE story file</action>
      <action>Extract story_key from filename or metadata</action>
      <goto> anchor with id task_check</goto>
    </check>

    <!-- Sprint-based story discovery -->
    <check if="{{sprint_status}} file exists">
      <critical>MUST read COMPLETE sprint-status.yaml file from start to end to preserve order</critical>
      <action>Load the FULL file: {{sprint_status}}</action>
      <action>Read ALL lines from beginning to end - do not skip any content</action>
      <action>Parse the development_status section completely to understand story order</action>

      <action>Find the FIRST story (by reading in order from top to bottom) where:
        - Key matches pattern: number-number-name (e.g., "1-2-user-auth")
        - NOT an epic key (epic-X) or retrospective (epic-X-retrospective)
        - Status value equals "ready-for-dev"
      </action>

      <check if="no ready-for-dev or in-progress story found">
        <output>üìã No ready-for-dev stories found in sprint-status.yaml

          **Current Sprint Status:** {{sprint_status_summary}}

          **What would you like to do?**
          1. Run `create-story` to create next story from epics with comprehensive context
          2. Run `*validate-create-story` to improve existing stories before development (recommended quality check)
          3. Specify a particular story file to develop (provide full path)
          4. Check {{sprint_status}} file to see current sprint status

          üí° **Tip:** Stories in `ready-for-dev` may not have been validated. Consider running `validate-create-story` first for a quality
          check.
        </output>
        <ask>Choose option [1], [2], [3], or [4], or specify story file path:</ask>

        <check if="user chooses '1'">
          <action>HALT - Run create-story to create next story</action>
        </check>

        <check if="user chooses '2'">
          <action>HALT - Run validate-create-story to improve existing stories</action>
        </check>

        <check if="user chooses '3'">
          <ask>Provide the story file path to develop:</ask>
          <action>Store user-provided story path as {{story_path}}</action>
          <goto anchor="task_check" />
        </check>

        <check if="user chooses '4'">
          <output>Loading {{sprint_status}} for detailed status review...</output>
          <action>Display detailed sprint status analysis</action>
          <action>HALT - User can review sprint status and provide story path</action>
        </check>

        <check if="user provides story file path">
          <action>Store user-provided story path as {{story_path}}</action>
          <goto anchor="task_check" />
        </check>
      </check>
    </check>

    <!-- Non-sprint story discovery -->
    <check if="{{sprint_status}} file does NOT exist">
      <action>Search {story_dir} for stories directly</action>
      <action>Find stories with "ready-for-dev" status in files</action>
      <action>Look for story files matching pattern: *-*-*.md</action>
      <action>Read each candidate story file to check Status section</action>

      <check if="no ready-for-dev stories found in story files">
        <output>üìã No ready-for-dev stories found

          **Available Options:**
          1. Run `create-story` to create next story from epics with comprehensive context
          2. Run `*validate-create-story` to improve existing stories
          3. Specify which story to develop
        </output>
        <ask>What would you like to do? Choose option [1], [2], or [3]:</ask>

        <check if="user chooses '1'">
          <action>HALT - Run create-story to create next story</action>
        </check>

        <check if="user chooses '2'">
          <action>HALT - Run validate-create-story to improve existing stories</action>
        </check>

        <check if="user chooses '3'">
          <ask>It's unclear what story you want developed. Please provide the full path to the story file:</ask>
          <action>Store user-provided story path as {{story_path}}</action>
          <action>Continue with provided story file</action>
        </check>
      </check>

      <check if="ready-for-dev story found in files">
        <action>Use discovered story file and extract story_key</action>
      </check>
    </check>

    <action>Store the found story_key (e.g., "1-2-user-authentication") for later status updates</action>
    <action>Find matching story file in {story_dir} using story_key pattern: {{story_key}}.md</action>
    <action>Read COMPLETE story file from discovered path</action>

    <anchor id="task_check" />

    <action>Parse sections: Story, Acceptance Criteria, Tasks/Subtasks, Dev Notes, Dev Agent Record, File List, Change Log, Status</action>

    <action>Load comprehensive context from story file's Dev Notes section</action>
    <action>Extract developer guidance from Dev Notes: architecture requirements, previous learnings, technical specifications</action>
    <action>Use enhanced story context to inform implementation decisions and approaches</action>

    <action>Identify first incomplete task (unchecked [ ]) in Tasks/Subtasks</action>

    <action if="no incomplete tasks">
      <goto step="6">Completion sequence</goto>
    </action>
    <action if="story file inaccessible">HALT: "Cannot develop story without access to story file"</action>
    <action if="incomplete task or subtask requirements ambiguous">ASK user to clarify or HALT</action>
  </step>

  <step n="2" goal="Load project context and story information">
    <critical>Load all available context to inform implementation</critical>

    <action>Load {project_context} for coding standards and project-wide patterns (if exists)</action>
    <action>Parse sections: Story, Acceptance Criteria, Tasks/Subtasks, Dev Notes, Dev Agent Record, File List, Change Log, Status</action>
    <action>Load comprehensive context from story file's Dev Notes section</action>
    <action>Extract developer guidance from Dev Notes: architecture requirements, previous learnings, technical specifications</action>
    <action>Use enhanced story context to inform implementation decisions and approaches</action>
    <output>‚úÖ **Context Loaded**
      Story and project context available for implementation
    </output>
  </step>

  <step n="3" goal="Detect review continuation and extract review context">
    <critical>Determine if this is a fresh start or continuation after code review</critical>

    <action>Check if "Senior Developer Review (AI)" section exists in the story file</action>
    <action>Check if "Review Follow-ups (AI)" subsection exists under Tasks/Subtasks</action>

    <check if="Senior Developer Review section exists">
      <action>Set review_continuation = true</action>
      <action>Extract from "Senior Developer Review (AI)" section:
        - Review outcome (Approve/Changes Requested/Blocked)
        - Review date
        - Total action items with checkboxes (count checked vs unchecked)
        - Severity breakdown (High/Med/Low counts)
      </action>
      <action>Count unchecked [ ] review follow-up tasks in "Review Follow-ups (AI)" subsection</action>
      <action>Store list of unchecked review items as {{pending_review_items}}</action>

      <output>‚èØÔ∏è **Resuming Story After Code Review** ({{review_date}})

        **Review Outcome:** {{review_outcome}}
        **Action Items:** {{unchecked_review_count}} remaining to address
        **Priorities:** {{high_count}} High, {{med_count}} Medium, {{low_count}} Low

        **Strategy:** Will prioritize review follow-up tasks (marked [AI-Review]) before continuing with regular tasks.
      </output>
    </check>

    <check if="Senior Developer Review section does NOT exist">
      <action>Set review_continuation = false</action>
      <action>Set {{pending_review_items}} = empty</action>

      <output>üöÄ **Starting Fresh Implementation**

        Story: {{story_key}}
        Story Status: {{current_status}}
        First incomplete task: {{first_task_description}}
      </output>
    </check>
  </step>

  <step n="4" goal="Mark story in-progress" tag="sprint-status">
    <check if="{{sprint_status}} file exists">
      <action>Load the FULL file: {{sprint_status}}</action>
      <action>Read all development_status entries to find {{story_key}}</action>
      <action>Get current status value for development_status[{{story_key}}]</action>

      <check if="current status == 'ready-for-dev' OR review_continuation == true">
        <action>Update the story in the sprint status report to = "in-progress"</action>
        <output>üöÄ Starting work on story {{story_key}}
          Status updated: ready-for-dev ‚Üí in-progress
        </output>
      </check>

      <check if="current status == 'in-progress'">
        <output>‚èØÔ∏è Resuming work on story {{story_key}}
          Story is already marked in-progress
        </output>
      </check>

      <check if="current status is neither ready-for-dev nor in-progress">
        <output>‚ö†Ô∏è Unexpected story status: {{current_status}}
          Expected ready-for-dev or in-progress. Continuing anyway...
        </output>
      </check>

      <action>Store {{current_sprint_status}} for later use</action>
    </check>

    <check if="{{sprint_status}} file does NOT exist">
      <output>‚ÑπÔ∏è No sprint status file exists - story progress will be tracked in story file only</output>
      <action>Set {{current_sprint_status}} = "no-sprint-tracking"</action>
    </check>
  </step>

  <step n="4b" goal="Create or link GitHub issue for traceability" tag="github-traceability">
    <critical>Ensure story is linked to a GitHub issue for commit traceability</critical>

    <!-- Check for existing GitHub issue -->
    <action>Search story file for existing "**GitHub Issue:**" field</action>

    <check if="GitHub Issue field exists and has valid issue number">
      <action>Extract issue number (e.g., #15 -> 15)</action>
      <action>Store {{github_issue_number}} for use in commits</action>
      <output>üîó Story linked to existing GitHub Issue #{{github_issue_number}}</output>
    </check>

    <check if="GitHub Issue field does NOT exist or is empty">
      <output>üìù No GitHub issue linked - creating one for traceability...</output>

      <!-- Extract repository info -->
      <action>Run: git remote get-url origin</action>
      <action>Parse github_owner and github_repo from remote URL</action>

      <!-- Extract story info for issue -->
      <action>Extract story title from # heading</action>
      <action>Extract story description (As a... I want... So that...)</action>
      <action>Extract acceptance criteria section</action>
      <action>Get relative path to story file from repository root</action>

      <!-- Determine labels -->
      <action>Extract epic number from story_key (e.g., "1-3-farmer-registration" -> "1")</action>
      <action>Set labels: ["story", "epic-{{epic_number}}"]</action>

      <!-- Create issue -->
      <action>Use mcp__github__create_issue with:
        - owner: {{github_owner}}
        - repo: {{github_repo}}
        - title: "Story {{story_key}}: {{story_title}}"
        - body: formatted with story description, acceptance criteria, and file path
        - labels: {{labels}}
      </action>

      <check if="issue creation succeeds">
        <action>Store returned issue number as {{github_issue_number}}</action>

        <!-- Update story file -->
        <action>Find "**Status:**" line in story file</action>
        <action>Add "**GitHub Issue:** #{{github_issue_number}}" on new line after Status</action>
        <action>Save story file</action>

        <!-- Update sprint-status if exists -->
        <check if="{{sprint_status}} file exists">
          <action>Add github_issue field to story entry in sprint-status.yaml</action>
        </check>

        <output>‚úÖ Created GitHub Issue #{{github_issue_number}}
          URL: https://github.com/{{github_owner}}/{{github_repo}}/issues/{{github_issue_number}}

          üìå **Commit Reference:** Include "Relates to #{{github_issue_number}}" in commit messages
          ‚ö†Ô∏è **IMPORTANT:** Do NOT use "Fixes #N" or "Closes #N" - these auto-close the issue!
             Use "Relates to #N" until story reaches "done" status.
        </output>
      </check>

      <check if="issue creation fails">
        <output>‚ö†Ô∏è Could not create GitHub issue (API error or missing permissions)
          Continuing without GitHub issue linking...
        </output>
        <action>Set {{github_issue_number}} = null</action>
      </check>
    </check>

    <!-- Store for later use in commits -->
    <action>Store {{github_issue_number}} in session for commit message formatting</action>
  </step>

  <step n="5" goal="Implement task following red-green-refactor cycle">
    <critical>FOLLOW THE STORY FILE TASKS/SUBTASKS SEQUENCE EXACTLY AS WRITTEN - NO DEVIATION</critical>

    <action>Review the current task/subtask from the story file - this is your authoritative implementation guide</action>
    <action>Plan implementation following red-green-refactor cycle</action>

    <!-- RED PHASE -->
    <action>Write FAILING tests first for the task/subtask functionality</action>
    <action>Confirm tests fail before implementation - this validates test correctness</action>

    <!-- GREEN PHASE -->
    <action>Implement MINIMAL code to make tests pass</action>
    <action>Run tests to confirm they now pass</action>
    <action>Handle error conditions and edge cases as specified in task/subtask</action>

    <!-- REFACTOR PHASE -->
    <action>Improve code structure while keeping tests green</action>
    <action>Ensure code follows architecture patterns and coding standards from Dev Notes</action>

    <action>Document technical approach and decisions in Dev Agent Record ‚Üí Implementation Plan</action>

    <action if="new dependencies required beyond story specifications">HALT: "Additional dependencies need user approval"</action>
    <action if="3 consecutive implementation failures occur">HALT and request guidance</action>
    <action if="required configuration is missing">HALT: "Cannot proceed without necessary configuration files"</action>

    <critical>NEVER implement anything not mapped to a specific task/subtask in the story file</critical>
    <critical>NEVER proceed to next task until current task/subtask is complete AND tests pass</critical>
    <critical>Execute continuously without pausing until all tasks/subtasks are complete or explicit HALT condition</critical>
    <critical>Do NOT propose to pause for review until Step 9 completion gates are satisfied</critical>
  </step>

  <step n="6" goal="Author comprehensive tests">
    <action>Create unit tests for business logic and core functionality introduced/changed by the task</action>
    <action>Add integration tests for component interactions specified in story requirements</action>
    <action>Include end-to-end tests for critical user flows when story requirements demand them</action>
    <action>Cover edge cases and error handling scenarios identified in story Dev Notes</action>
  </step>

  <step n="7" goal="Run validations and unit/integration tests">
    <action>Determine how to run tests for this repo (infer test framework from project structure)</action>
    <action>Run all existing unit tests to ensure no regressions</action>
    <action>Run the new tests to verify implementation correctness</action>
    <action>Run linting and code quality checks if configured in project</action>
    <action>Validate implementation meets ALL story acceptance criteria; enforce quantitative thresholds explicitly</action>
    <action if="regression tests fail">STOP and fix before continuing - identify breaking changes immediately</action>
    <action if="new tests fail">STOP and fix before continuing - ensure implementation correctness</action>
  </step>

  <step n="7b" goal="Run E2E tests locally (MANDATORY)">
    <critical>E2E tests MUST pass locally before marking story ready for review - NO EXCEPTIONS</critical>
    <critical>This step CANNOT be skipped - story will be rejected without E2E evidence</critical>
    <critical>Docker images MUST be rebuilt before running E2E tests - stale images cause false positives</critical>

    <action>Check if E2E test infrastructure exists: tests/e2e/infrastructure/docker-compose.e2e.yaml</action>

    <check if="E2E infrastructure exists">
      <output>üß™ **Starting E2E Test Suite (MANDATORY)**
        E2E tests must pass before story can move to review status.

        ‚ö†Ô∏è **IMPORTANT:** Docker images will be rebuilt to ensure you're testing current code.
      </output>

      <action>Rebuild and start E2E infrastructure (--build is MANDATORY to avoid testing stale images):
        docker compose -f tests/e2e/infrastructure/docker-compose.e2e.yaml up -d --build
      </action>
      <action>Wait for services to be healthy (use docker compose logs to verify)</action>
      <action>Run E2E test suite:
        PYTHONPATH="${PYTHONPATH}:.:libs/fp-proto/src" pytest tests/e2e/scenarios/ -v
      </action>
      <action>Capture COMPLETE E2E output including pass/fail counts</action>
      <action>Tear down infrastructure:
        docker compose -f tests/e2e/infrastructure/docker-compose.e2e.yaml down -v
      </action>

      <action if="E2E tests fail">HALT: "E2E tests failed - MUST fix before proceeding to review"</action>

      <action>Paste E2E output in story file under "Local Test Run Evidence ‚Üí E2E Tests" section</action>
      <action>Mark E2E evidence checkbox in story file</action>

      <output>‚úÖ **E2E Tests Passed**
        Evidence captured in story file. Proceeding to task completion.
      </output>
    </check>

    <check if="E2E infrastructure does NOT exist">
      <output>‚ÑπÔ∏è No E2E infrastructure configured for this project - skipping E2E step</output>
    </check>
  </step>

  <step n="8" goal="Validate and mark task complete ONLY when fully done">
    <critical>NEVER mark a task complete unless ALL conditions are met - NO LYING OR CHEATING</critical>

    <!-- VALIDATION GATES -->
    <action>Verify ALL tests for this task/subtask ACTUALLY EXIST and PASS 100%</action>
    <action>Confirm implementation matches EXACTLY what the task/subtask specifies - no extra features</action>
    <action>Validate that ALL acceptance criteria related to this task are satisfied</action>
    <action>Run full test suite to ensure NO regressions introduced</action>

    <!-- REVIEW FOLLOW-UP HANDLING -->
    <check if="task is review follow-up (has [AI-Review] prefix)">
      <action>Extract review item details (severity, description, related AC/file)</action>
      <action>Add to resolution tracking list: {{resolved_review_items}}</action>

      <!-- Mark task in Review Follow-ups section -->
      <action>Mark task checkbox [x] in "Tasks/Subtasks ‚Üí Review Follow-ups (AI)" section</action>

      <!-- CRITICAL: Also mark corresponding action item in review section -->
      <action>Find matching action item in "Senior Developer Review (AI) ‚Üí Action Items" section by matching description</action>
      <action>Mark that action item checkbox [x] as resolved</action>

      <action>Add to Dev Agent Record ‚Üí Completion Notes: "‚úÖ Resolved review finding [{{severity}}]: {{description}}"</action>
    </check>

    <!-- ONLY MARK COMPLETE IF ALL VALIDATION PASS -->
    <check if="ALL validation gates pass AND tests ACTUALLY exist and pass">
      <action>ONLY THEN mark the task (and subtasks) checkbox with [x]</action>
      <action>Update File List section with ALL new, modified, or deleted files (paths relative to repo root)</action>
      <action>Add completion notes to Dev Agent Record summarizing what was ACTUALLY implemented and tested</action>
    </check>

    <check if="ANY validation fails">
      <action>DO NOT mark task complete - fix issues first</action>
      <action>HALT if unable to fix validation failures</action>
    </check>

    <check if="review_continuation == true and {{resolved_review_items}} is not empty">
      <action>Count total resolved review items in this session</action>
      <action>Add Change Log entry: "Addressed code review findings - {{resolved_count}} items resolved (Date: {{date}})"</action>
    </check>

    <action>Save the story file</action>
    <action>Determine if more incomplete tasks remain</action>
    <action if="more tasks remain">
      <goto step="5">Next task</goto>
    </action>
    <action if="no tasks remain">
      <goto step="9">Completion</goto>
    </action>
  </step>

  <step n="9" goal="Story completion and mark for review" tag="sprint-status">
    <action>Verify ALL tasks and subtasks are marked [x] (re-scan the story document now)</action>
    <action>Run the full regression suite (do not skip)</action>
    <action>Confirm File List includes every changed file</action>
    <action>Execute enhanced definition-of-done validation</action>
    <action>Update the story Status to: "review"</action>

    <!-- Enhanced Definition of Done Validation -->
    <action>Validate definition-of-done checklist with essential requirements:
      - All tasks/subtasks marked complete with [x]
      - Implementation satisfies every Acceptance Criterion
      - Unit tests for core functionality added/updated
      - Integration tests for component interactions added when required
      - End-to-end tests for critical flows added when story demands them
      - All tests pass (no regressions, new tests successful)
      - Code quality checks pass (linting, static analysis if configured)
      - File List includes every new/modified/deleted file (relative paths)
      - Dev Agent Record contains implementation notes
      - Change Log includes summary of changes
      - Only permitted story sections were modified
    </action>

    <!-- Mark story ready for review - sprint status conditional -->
    <check if="{sprint_status} file exists AND {{current_sprint_status}} != 'no-sprint-tracking'">
      <action>Load the FULL file: {sprint_status}</action>
      <action>Find development_status key matching {{story_key}}</action>
      <action>Verify current status is "in-progress" (expected previous state)</action>
      <action>Update development_status[{{story_key}}] = "review"</action>
      <action>Save file, preserving ALL comments and structure including STATUS DEFINITIONS</action>
      <output>‚úÖ Story status updated to "review" in sprint-status.yaml</output>
    </check>

    <check if="{sprint_status} file does NOT exist OR {{current_sprint_status}} == 'no-sprint-tracking'">
      <output>‚ÑπÔ∏è Story status updated to "review" in story file (no sprint tracking configured)</output>
    </check>

    <check if="story key not found in sprint status">
      <output>‚ö†Ô∏è Story file updated, but sprint-status update failed: {{story_key}} not found

        Story status is set to "review" in file, but sprint-status.yaml may be out of sync.
      </output>
    </check>
  </step>

  <step n="9b" goal="Push to story branch and verify Quality CI passes (MANDATORY)">
    <critical>Quality CI (ci.yaml) MUST pass on the story branch before proceeding</critical>
    <critical>Quality CI is AUTO-TRIGGERED on push - verify it completes successfully</critical>

    <!-- Push changes to story branch -->
    <action>Ensure all changes are committed to the story branch</action>
    <action>Push to remote story branch:
      git push origin {{story_branch}}
    </action>

    <output>üöÄ **Pushed to Story Branch**
      Branch: {{story_branch}}
      Waiting for Quality CI (ci.yaml) to complete...
    </output>

    <!-- Wait and verify Quality CI -->
    <action>Wait ~30 seconds for Quality CI to start</action>
    <action>Check Quality CI status on story branch:
      gh run list --workflow=ci.yaml --branch {{story_branch}} --limit 1
    </action>

    <check if="Quality CI run is in progress">
      <output>‚è≥ **Quality CI Running on Story Branch**
        Workflow: ci.yaml
        Run: {{quality_ci_run_id}}
        Status: In Progress

        Wait for completion before proceeding...
      </output>
      <action>Monitor Quality CI status until complete:
        gh run watch {{quality_ci_run_id}}
      </action>
    </check>

    <check if="Quality CI passed">
      <output>‚úÖ **Quality CI Passed**
        Workflow: ci.yaml
        Branch: {{story_branch}}
        Run: {{quality_ci_run_id}}
        Status: PASSED (lint, format, unit tests)

        Proceeding to E2E CI verification...
      </output>
      <action>Record Quality CI run ID in story file Dev Agent Record</action>
    </check>

    <check if="Quality CI failed">
      <action>Get failure details:
        gh run view {{quality_ci_run_id}} --log-failed
      </action>
      <output>‚ùå **Quality CI Failed on Story Branch**
        Workflow: ci.yaml
        Branch: {{story_branch}}
        Run: {{quality_ci_run_id}}

        **Failure Details:**
        {{failure_logs}}

        HALT: Fix Quality CI failures before proceeding.
      </output>
      <action>HALT: "Quality CI (ci.yaml) failed - fix before proceeding"</action>
    </check>

    <!-- Update story file with Quality CI evidence -->
    <action>Add Quality CI verification evidence to story file:
      - Branch: {{story_branch}}
      - Workflow: ci.yaml
      - Quality CI Run ID: {{quality_ci_run_id}}
      - Quality CI Status: PASSED
      - Verification Date: {{date}}
    </action>
  </step>

  <step n="9c" goal="Trigger and verify E2E CI passes (MANDATORY - MANUAL TRIGGER)">
    <critical>E2E CI (e2e.yaml) MUST be MANUALLY TRIGGERED - it does NOT auto-run on push</critical>
    <critical>You MUST trigger, wait, and verify E2E CI passes before proceeding</critical>
    <critical>This step CANNOT be skipped - story will be rejected without E2E CI evidence</critical>

    <output>üß™ **E2E CI Verification (MANUAL TRIGGER REQUIRED)**
      E2E workflow must be manually triggered on the story branch.
      This is separate from Quality CI which runs automatically.
    </output>

    <!-- Step 1: Trigger E2E workflow -->
    <action>Trigger E2E CI workflow on current branch:
      gh workflow run e2e.yaml --ref {{story_branch}}
    </action>

    <output>üöÄ **E2E CI Triggered**
      Workflow: e2e.yaml
      Branch: {{story_branch}}
      Waiting for workflow to start...
    </output>

    <!-- Step 2: Wait for workflow to appear and get run ID -->
    <action>Wait for E2E workflow run to appear (~5-10 seconds):
      sleep 10
      gh run list --workflow=e2e.yaml --branch {{story_branch}} --limit 1 --json databaseId,status,conclusion
    </action>

    <!-- Step 3: Monitor until completion -->
    <check if="E2E CI run is in progress">
      <output>‚è≥ **E2E CI Running**
        Workflow: e2e.yaml
        Run: {{e2e_run_id}}
        Status: In Progress
        Expected duration: 3-5 minutes

        Monitoring until completion...
      </output>
      <action>Monitor E2E CI until complete (timeout 10 minutes):
        gh run watch {{e2e_run_id}}
      </action>
    </check>

    <!-- Step 4: Verify final status -->
    <check if="E2E CI passed">
      <action>Get E2E run details:
        gh run view {{e2e_run_id}}
      </action>
      <output>‚úÖ **E2E CI Passed**
        Workflow: e2e.yaml
        Branch: {{story_branch}}
        Run: {{e2e_run_id}}
        Status: PASSED

        Both Quality CI and E2E CI have passed. Story is ready for code review.
      </output>
      <action>Record E2E CI run ID in story file Dev Agent Record</action>
    </check>

    <check if="E2E CI failed">
      <action>Get failure details:
        gh run view {{e2e_run_id}} --log-failed
      </action>
      <output>‚ùå **E2E CI Failed**
        Workflow: e2e.yaml
        Branch: {{story_branch}}
        Run: {{e2e_run_id}}

        **Failure Details:**
        {{failure_logs}}

        HALT: Fix E2E CI failures before story can proceed.
      </output>
      <action>HALT: "E2E CI (e2e.yaml) failed - fix before proceeding"</action>
    </check>

    <check if="E2E workflow trigger failed">
      <output>‚ùå **E2E CI Trigger Failed**
        Could not trigger e2e.yaml workflow.

        **Possible causes:**
        - Workflow file does not exist at .github/workflows/e2e.yaml
        - Workflow missing `workflow_dispatch` trigger
        - Insufficient permissions

        HALT: Investigate E2E workflow configuration.
      </output>
      <action>HALT: "Cannot trigger E2E CI - check workflow configuration"</action>
    </check>

    <!-- Update story file with E2E CI evidence -->
    <action>Add E2E CI verification evidence to story file:
      - Branch: {{story_branch}}
      - Workflow: e2e.yaml
      - E2E CI Run ID: {{e2e_run_id}}
      - E2E CI Status: PASSED
      - Verification Date: {{date}}
    </action>
  </step>

  <step n="9d" goal="Update GitHub issue with development completion" tag="github-update">
    <!-- Update GitHub issue with development completion -->
    <check if="{{github_issue_number}} exists and is not null">
      <action>Add comment to GitHub issue #{{github_issue_number}} with development completion summary:
        - Status: Development Complete - Ready for Review
        - Tasks completed (count)
        - Tests added/updated (summary)
        - Files modified (list from File List section)
        - Acceptance criteria addressed
      </action>
      <output>üìù GitHub issue #{{github_issue_number}} updated with development completion status</output>
    </check>

    <check if="{{github_issue_number}} is null or does not exist">
      <output>‚ÑπÔ∏è No GitHub issue linked - skipping issue update</output>
    </check>

    <!-- Final validation gates -->
    <action if="any task is incomplete">HALT - Complete remaining tasks before marking ready for review</action>
    <action if="regression failures exist">HALT - Fix regression issues before completing</action>
    <action if="File List is incomplete">HALT - Update File List with all changed files</action>
    <action if="definition-of-done validation fails">HALT - Address DoD failures before completing</action>
  </step>

  <step n="9e" goal="Trigger mandatory code review (REQUIRED)">
    <critical>Code review MUST be executed before story can be marked done - NO EXCEPTIONS</critical>
    <critical>This step CANNOT be skipped - story will be rejected without code review evidence</critical>

    <action>Verify story status is "review" (set in Step 9)</action>

    <output>üìã **MANDATORY CODE REVIEW REQUIRED**

      Story implementation is complete. Code review is REQUIRED before story can move to "done".

      **Current Status:** review (awaiting code review)

      **Required Action:** Run `/code-review` workflow

      ‚õî **BLOCKED:** Story CANNOT move to "done" status until:
      - Code review workflow has been executed
      - All review findings have been addressed (or explicitly waived)
      - Review evidence is captured in story file

      üí° **Tip:** For best results, run code-review using a **different** LLM than the one that implemented this story.
    </output>

    <action>HALT: "Run /code-review workflow - story cannot be completed without code review"</action>
  </step>

  <step n="10" goal="Completion communication and user support">
    <action>Execute the enhanced definition-of-done checklist using the validation framework</action>
    <action>Prepare a concise summary in Dev Agent Record ‚Üí Completion Notes</action>

    <action>Communicate to {user_name} that story implementation is complete and ready for review</action>
    <action>Summarize key accomplishments: story ID, story key, title, key changes made, tests added, files modified</action>
    <action>Provide the story file path and current status (now "review")</action>

    <action>Based on {user_skill_level}, ask if user needs any explanations about:
      - What was implemented and how it works
      - Why certain technical decisions were made
      - How to test or verify the changes
      - Any patterns, libraries, or approaches used
      - Anything else they'd like clarified
    </action>

    <check if="user asks for explanations">
      <action>Provide clear, contextual explanations tailored to {user_skill_level}</action>
      <action>Use examples and references to specific code when helpful</action>
    </check>

    <action>Once explanations are complete (or user indicates no questions), suggest logical next steps</action>
    <action>Recommended next steps (flexible based on project setup):
      - Review the implemented story and test the changes
      - Verify all acceptance criteria are met
      - Ensure deployment readiness if applicable
      - Run `code-review` workflow for peer review
    </action>

    <output>üí° **Tip:** For best results, run `code-review` using a **different** LLM than the one that implemented this story.</output>
    <check if="{sprint_status} file exists">
      <action>Suggest checking {sprint_status} to see project progress</action>
    </check>
    <action>Remain flexible - allow user to choose their own path or ask for other assistance</action>
  </step>

</workflow>